snippet hive_init1 "initialize hive context" b
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.log4j.Level
import org.apache.log4j.Logger

object ${1:Demo} {
	def main(args: Array[String]): Unit = {
		Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
		Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.ERROR)

		if (args.length != ${2:1}) {
			System.err.println("usage: $1 <${3:timestamp}>")
			System.exit(1)
		}

		val ${4:inputTime} = args(0)

		val conf = new SparkConf().setAppName("$1")
		val sc = new SparkContext(conf)

		val hiveContext = new HiveContext(sc)
		import hiveContext.implicits._
		/*
		hiveContext.setConf("hive.exec.dynamic.partition", "true")
		hiveContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")
		*/
		}
}
endsnippet

snippet hive_init2 "initialize hive context" b
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.log4j.Level
import org.apache.log4j.Logger

object ${1:Demo} {
	def main(args: Array[String]): Unit = {
		Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
		Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.ERROR)

		if (args.length != ${2:1}) {
			System.err.println("usage: $1 <${3:timestamp}>")
			System.exit(1)
		}

		val ${4:inputTime} = args(0)
		val spark = SparkSession
			.builder()
			.appName("$1")
			.config("hive.exec.dynamic.partition", "true")
			.config("hive.exec.dynamic.partition.mode", "nonstrict")
			.getOrCreate()

		// For implicit conversions like converting RDDs to DataFrames
		import spark.implicits._
		}
}
endsnippet

snippet df_writer "dataframe writer"
val ${1:writer} = ${2:df}.write.mode("${3:overwrite}").format("${4:ORC}")
endsnippet

snippet df_reader "dataframe reader"
val ${1:dfReader} = hiveContext.read.format("${4:ORC}")
endsnippet

snippet df_agg "dataframe agg"
${1:first}("${2:column_name}").alias("$2")
endsnippet

snippet df_first "first"
first("${1:column_name}").alias("$1")
endsnippet

snippet df_udf "dataframe udf" b
val ${1} = udf((${2}: ${2}) => {
	${0}
})
endsnippet

snippet df_add_timestamp "dataframe add timestamp" b
withColumn("timestamp", unix_timestamp(lit(timestamp), "yyyyMMddHHmm").cast(TimestampType)).
withColumn("update_time", current_timestamp())
endsnippet

snippet df_window "dataframe window" b
import org.apache.spark.sql.expressions.Window
val window = Window.partitionBy(${1}).orderBy(col(${2}).${3:desc})
df.withColumn("${4:rank}", ${0:dense_rank()}.over(window)))
endsnippet

snippet df_empty "create empty dataframe" b
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
val ${1:df} = hiveContext.createDataFrame(sc.emptyRDD[Row], StructType(Seq()))
endsnippet

snippet df_empty_same_schema "create empty dataframe in same schema for unionAll" b
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
val ${1:df} = hiveContext.createDataFrame(sc.emptyRDD[Row], ${2:oldDF}.schema)
endsnippet

snippet df_empty_plus_schema "create empty dataframe in plus schema" b
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
val ${1:df} = hiveContext.createDataFrame(sc.emptyRDD[Row], StructType(${2:oldDF}.schema.fields ++ Array(StructField(${3:field}, ${4:StringType}, true))))
endsnippet
