snippet streaming_init "initialize streaming"
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._
import org.apache.spark.storage.StorageLevel

import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Durations

import org.apache.log4j.Level
import org.apache.log4j.Logger

object ${1:Demo} {

  def main(args: Array[String]): Unit = {
    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.ERROR)

    if (args.length != ${2:1}) {
      System.err.println("usage: $1 <${3:timestamp}>")
      System.exit(1)
    }

    val ${4:inputTime} = args(0)

    val conf = new SparkConf().setAppName("${5:Demo}")
    conf.set("spark.driver.allowMultipleContexts", "true");
    conf.set("spark.streaming.backpressure.enabled", "true");
    conf.set("spark.streaming.backpressure.initialRate", "10000");
    conf.set("spark.task.maxFailures", "10");

    val sc = new SparkContext(conf)
    val ssc = new StreamingContext(sc, Duration.seconds(${6:60}))
    // val ssc = new StreamingContext(conf, Seconds($6))
    // scc.sparkContext.setLogLevel("ERROR")
    // scc.checkpoint(".")

    ssc.start()
    ssc.awaitTermination()
    ssc.stop()
  }
}
endsnippet


snippet streaming_hive_init "initialize streaming hive"
import org.apache.spark.sql.hive.HiveContext

object SQLHiveContextSingleton {
  @transient private var instance: HiveContext = _
  def getInstance(sparkContext: SparkContext): HiveContext = {
    synchronized {
      if (instance == null ) {
        instance = new HiveContext(sparkContext)
      }
      instance
    }
  }
}
endsnippet

snippet streaming_kafka_init "initialize streaming kafka"
import org.apache.spark.streaming.kafka.KafkaUtils
import kafka.serializer.StringDecoder

// initialize streaming kafka
val kafkaParam = Map[String, String](
  "zookeeper.connect" -> "${1:127.0.0.1:2181}",
  "group.id" -> "${2:test-consumer-group}",
  "metadata.broker.list" -> "${3:127.0.0.1:9092}"
  "serializer.class" -> "kafka.serializer.StringEncoder"
)
val topics = Set("${4:kafka-spark-demo}")
// val topics = Map[String, Integer]("kafka-spark-demo" -> 30)

val linerdd = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
  ssc, kafkaParams, topics)
//kafka返回的数据时key/value形式，后面只要对value进行分割就ok了
val wordrdd = linerdd.flatMap { _._2.split(" ") }
wordrdd.foreachRDD(rdd => {
  println("从topic:" + topics + "读取rdd:" + rdd.count())
})
wordrdd.print()
val resultrdd = wordrdd.map { x => (x, 1) }.reduceByKey { _ + _ }
resultrdd.print()
endsnippet

snippet streaming_kafka_producer "streaming kafka producer"
import java.util.Properties;
import java.util.concurrent.TimeUnit;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
import kafka.serializer.StringEncoder;

public class kafkaProducer extends Thread {

    private String topic;

    public kafkaProducer(String topic){
        super();
        this.topic = topic;
    }

    @Override
    public void run() {
        Producer producer = createProducer();
        int i=0;
        while(true){
            producer.send(new KeyedMessage<Integer, String>(topic, "message: " + i++));
            try {
                TimeUnit.SECONDS.sleep(1);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    private Producer createProducer() {
        Properties properties = new Properties();
        // 配置broker和zookeeper
        properties.put("metadata.broker.list", "127.0.0.1:9092");
        properties.put("zk.connect", "127.0.0.1:2181");

        // 配置value的序列化类
        properties.put("serializer.class", "kafka.serializer.StringEncoder");
        // 配置key的序列化类
        properties.put("key.serializer.class", "kafka.serializer.StringEncoder");

        properties.put("request.required.acks", "-1");

        return new Producer<Integer, String>(new ProducerConfig(properties));
    }

    public static void main(String[] args) {
        new kafkaProducer("${1:kafka-spark-demo}").start();

    }
}
endsnippet
